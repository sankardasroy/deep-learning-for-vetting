{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Embedding,Dense,Flatten\n",
    "from keras.layers import Bidirectional\n",
    "#from keras.models import load_model\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import math\n",
    "input_path = \"/home/ubuntu/static_mixed\"\n",
    "word2vec_model = KeyedVectors.load(\"/home/ubuntu/first.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={}\n",
    "def create_label():\n",
    "    \n",
    "    with open(\"/home/ubuntu/composite.txt\") as file:\n",
    "        for _ in range(18227):\n",
    "            line  = file.readline().strip().split(\",\")\n",
    "            label_dict[line[0]] = line[1]\n",
    "create_label()\n",
    "\n",
    "#print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#HERE START GENERATOR THINGY\n",
    "import os\n",
    "import random\n",
    "#nb_words=len(word2vec_model.wv.vocab), will be parameter for tokenizer\n",
    "tokenizer = Tokenizer(filters='#\\n')\n",
    "tokenizer.fit_on_texts(word2vec_model.wv.vocab.keys())\n",
    "word_index = tokenizer.word_index\n",
    "temp_array=[]\n",
    "batch_size = 75\n",
    "def get_content(file_names):\n",
    "    c=0 \n",
    "    temp_array=[]\n",
    "    label_array=[]\n",
    "    while True:\n",
    "        if c>=len(file_names):\n",
    "            c=0\n",
    "        for i,j in enumerate(file_names[c:c+batch_size]):\n",
    "            l = open(\"/home/ubuntu/static_mixed/\"+j,'r').read().strip()\n",
    " \n",
    "            padded_sequence =  sequence.pad_sequences(tokenizer.texts_to_sequences([l]),maxlen=4000, padding='post',truncating='post')\n",
    "\n",
    "            temp_array.append(padded_sequence[0])\n",
    "            if int(label_dict.get(j))==1:\n",
    "                label_array.append([0,1])\n",
    "            else:\n",
    "                label_array.append([1,0])\n",
    "\n",
    "        temp_array = np.array(temp_array)\n",
    "        label_array = np.array(label_array)\n",
    "        #print(\"\\nYIELDING FROM c = \",c,\" c+batch_size = \",c+batch_size,\" and length of temp_array = \",len(temp_array),len(label_array))\n",
    "        yield (temp_array, label_array)\n",
    "        label_array=[]\n",
    "        temp_array=[]\n",
    "        c+=batch_size\n",
    "files_ = os.listdir(\"/home/ubuntu/static_mixed\")\n",
    "random.shuffle(files_)\n",
    "train_data = get_content(files_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4000, 100)         9848200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               234496    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 10,083,210\n",
      "Trainable params: 235,010\n",
      "Non-trainable params: 9,848,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word2vec_model.wv.vocab)+1,100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i]=word2vec_model.wv[word]\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word2vec_model.wv.vocab)+1,100,weights=[embedding_matrix],input_length=4000))\n",
    "model.add(Bidirectional(LSTM(128,return_sequences=False)))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.layers[0].trainable = False\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=load_model(\"bi_lstm_model.h5\")\n",
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.01),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/6\n",
      "204/204 [==============================] - 1859s 9s/step - loss: 0.2296 - acc: 0.9139\n",
      "Epoch 2/6\n",
      "204/204 [==============================] - 1860s 9s/step - loss: 0.1462 - acc: 0.9514\n",
      "Epoch 3/6\n",
      " 37/204 [====>.........................] - ETA: 25:20 - loss: 0.1265 - acc: 0.9553"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = math.ceil(len(files_)/batch_size)\n",
    "model.fit_generator(train_data,epochs=6,steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('bi_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALidation generator\n",
    "\n",
    "validation_size = 50\n",
    "def get_validation_data(file_names):\n",
    "    c=0 \n",
    "    temp_array=[]\n",
    "    \n",
    "    while True:\n",
    "        if c>=len(file_names):\n",
    "            c=0\n",
    "        for i,j in enumerate(file_names[c:c+validation_size]):\n",
    "            l = open(\"/home/ubuntu/static_validation/\"+j,'r').read().strip()\n",
    "            padded_sequence =  sequence.pad_sequences(tokenizer.texts_to_sequences([l]),maxlen=4000,padding='post',truncating='post')\n",
    "            temp_array.append(padded_sequence[0])\n",
    "            \n",
    "\n",
    "        temp_array = np.array(temp_array)\n",
    "        #label_array = np.array(label_array)\n",
    "        #temp_array = temp_array[..., np.newaxis]\n",
    "        #print(\"\\nYIELDING FROM c = \",c,\" c+validation_size = \",c+validation_size,\" and length of temp_array = \",len(temp_array))\n",
    "        yield (temp_array)\n",
    "        \n",
    "        temp_array=[]\n",
    "        c+=validation_size\n",
    "list_of_validation_files = os.listdir(\"/home/ubuntu/static_validation\")\n",
    "#print(list_of_validation_files)\n",
    "validation_generator = get_validation_data(list_of_validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(math.ceil(len(list_of_validation_files)/validation_size))\n",
    "# raise\n",
    "prediction = model.predict_generator(validation_generator,math.ceil(len(list_of_validation_files)/validation_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",verticalalignment=\"top\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "actual_labels=[]\n",
    "for i,j in enumerate(list_of_validation_files):\n",
    "    if int(label_dict.get(j))==1:\n",
    "                actual_labels.append(1)\n",
    "    else:\n",
    "                actual_labels.append(0)\n",
    "#print(actual_labels)\n",
    "#print(prediction)\n",
    "prediction1 = np.argmax(prediction, axis=1) \n",
    "print(prediction1)\n",
    "\n",
    "cm = confusion_matrix(actual_labels, prediction1)\n",
    "plot_confusion_matrix(cm,['benign','malicious'],title='CONFUSION MATRIX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from funcsigs import signature\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from inspect import signature\n",
    "\n",
    "def call_precision_recall_curve(truelabel,predictedlabel,actualprediction,label):\n",
    "    \n",
    "    #print(truelabel.shape)\n",
    "    #print(actualprediction.shape)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(truelabel,actualprediction)\n",
    "    au = auc(recall, precision)\n",
    "    print(\"Area under graph \"+str(au))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    rec_score = recall_score(truelabel,predictedlabel,pos_label=1,average='binary')\n",
    "    precise_score = precision_score(truelabel,predictedlabel,pos_label=1,average='binary')\n",
    "    f1 = f1_score(truelabel,predictedlabel,pos_label=1, average='binary')\n",
    "    print(\"Precision for Malicious apps \"+str(precise_score))\n",
    "    print(\"Recall for Malicious apps \"+str(rec_score))\n",
    "    print(\"F1-score for Malicious apps  \" + str(f1))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    rec_score = recall_score(truelabel,predictedlabel,pos_label=0,average='binary')\n",
    "    precise_score = precision_score(truelabel,predictedlabel,pos_label=0,average='binary')\n",
    "    f1 = f1_score(truelabel,predictedlabel,pos_label=0, average='binary')\n",
    "    print(\"Precision for Benign apps \"+str(precise_score))\n",
    "    print(\"Recall for Benign apps \"+str(rec_score))\n",
    "    print(\"F1-score for Benign apps  \" + str(f1))\n",
    "    \n",
    "    \n",
    "    #plot the no-skill line too\n",
    "    positive_cases = sum(truelabel)/len(truelabel)\n",
    "    plt.plot([0, 1], [positive_cases, positive_cases], linestyle='--')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve')\n",
    "prediction_for_1=[]\n",
    "for each_prediction in prediction:\n",
    "    prediction_for_1.append(each_prediction[1])\n",
    "call_precision_recall_curve(np.array(actual_labels),np.array(prediction1),np.array(prediction_for_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
