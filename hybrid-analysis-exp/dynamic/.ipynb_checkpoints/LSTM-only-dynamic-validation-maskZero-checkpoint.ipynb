{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Embedding,Dense,Flatten\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from keras.models import load_model\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import os,sys\n",
    "import math\n",
    "input_path = \"/home/ubuntu/dynamic_mixed\"\n",
    "word2vec_model = KeyedVectors.load(\"/home/ubuntu/dynamic_word.model\", mmap='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={}\n",
    "def create_label():\n",
    "    \n",
    "    with open(\"/home/ubuntu/composite_dynamic.txt\") as file:\n",
    "        for _ in range(18227):\n",
    "            line  = file.readline().strip().split(\",\")\n",
    "            label_dict[line[0]] = line[1]\n",
    "create_label()\n",
    "\n",
    "#print(len(label_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#HERE START GENERATOR THINGY\n",
    "import os\n",
    "import random\n",
    "#nb_words=len(word2vec_model.wv.vocab), will be parameter for tokenizer\n",
    "tokenizer = Tokenizer(filters='#\\n')\n",
    "tokenizer.fit_on_texts(word2vec_model.wv.vocab.keys())\n",
    "word_index = tokenizer.word_index\n",
    "temp_array=[]\n",
    "batch_size = 75\n",
    "def get_content(file_names,train_indices):\n",
    "    c=0 \n",
    "    temp_array=[]\n",
    "    label_array=[]\n",
    "    while True:\n",
    "        if c>=len(train_indices):\n",
    "            c=0\n",
    "        for i,j in enumerate(train_indices[c:c+batch_size]):\n",
    "            l = open(\"/home/ubuntu/dynamic_mixed/\"+file_names[j],'r').read().strip()\n",
    " \n",
    "            padded_sequence =  sequence.pad_sequences(tokenizer.texts_to_sequences([l]),maxlen=4000, padding='post',truncating='post')\n",
    "\n",
    "            temp_array.append(padded_sequence[0])\n",
    "            try:\n",
    "                if int(label_dict.get(file_names[j]))==1:\n",
    "                    label_array.append([0,1])\n",
    "                else:\n",
    "                    label_array.append([1,0])\n",
    "            except:\n",
    "                print(\"Problem is \",j)\n",
    "                raise\n",
    "        temp_array = np.array(temp_array)\n",
    "        label_array = np.array(label_array)\n",
    "        #print(\"\\nYIELDING FROM c = \",c,\" c+batch_size = \",c+batch_size,\" and length of temp_array = \",len(temp_array),len(label_array))\n",
    "        yield (temp_array, label_array)\n",
    "        label_array=[]\n",
    "        temp_array=[]\n",
    "        c+=batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALidation generator\n",
    "\n",
    "validation_size = 50\n",
    "def get_validation_data(file_names,validation_indices):\n",
    "    c=0 \n",
    "    temp_array=[]\n",
    "    \n",
    "    while True:\n",
    "        if c>=len(validation_indices):\n",
    "            c=0\n",
    "        for i,j in enumerate(validation_indices[c:c+validation_size]):\n",
    "            l = open(\"/home/ubuntu/dynamic_mixed/\"+file_names[j],'r').read().strip()\n",
    "            padded_sequence =  sequence.pad_sequences(tokenizer.texts_to_sequences([l]),maxlen=4000,padding='post',truncating='post')\n",
    "            temp_array.append(padded_sequence[0])\n",
    "            \n",
    "\n",
    "        temp_array = np.array(temp_array)\n",
    "        #label_array = np.array(label_array)\n",
    "        #temp_array = temp_array[..., np.newaxis]\n",
    "        #print(\"\\nYIELDING FROM c = \",c,\" c+validation_size = \",c+validation_size,\" and length of temp_array = \",len(temp_array))\n",
    "        yield (temp_array)\n",
    "        \n",
    "        temp_array=[]\n",
    "        c+=validation_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word2vec_model.wv.vocab)+1,100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i]=word2vec_model.wv[word]\n",
    "\n",
    "def get_model(learning_rate,hidden_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word2vec_model.wv.vocab)+1,100,weights=[embedding_matrix],input_length=4000))\n",
    "    model.add(LSTM(hidden_units,return_sequences=False))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    model.layers[0].trainable = False\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=learning_rate),metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "def plot_confusion_matrix(counter,cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",verticalalignment=\"top\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Confusion-Matrix-Dynamic-'+str(counter)+'.png')\n",
    "    plt.tight_layout()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from funcsigs import signature\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from inspect import signature\n",
    "\n",
    "def call_precision_recall_curve(counter,truelabel,predictedlabel,actualprediction,label):\n",
    "    \n",
    "    #print(truelabel.shape)\n",
    "    #print(actualprediction.shape)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(truelabel,actualprediction)\n",
    "    au = auc(recall, precision)\n",
    "    print(\"Area under graph \"+str(au))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    rec_score = recall_score(truelabel,predictedlabel,pos_label=1,average='binary')\n",
    "    precise_score = precision_score(truelabel,predictedlabel,pos_label=1,average='binary')\n",
    "    f1 = f1_score(truelabel,predictedlabel,pos_label=1, average='binary')\n",
    "    print(\"Precision for Malicious apps \"+str(precise_score))\n",
    "    print(\"Recall for Malicious apps \"+str(rec_score))\n",
    "    print(\"F1-score for Malicious apps  \" + str(f1))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    rec_score = recall_score(truelabel,predictedlabel,pos_label=0,average='binary')\n",
    "    precise_score = precision_score(truelabel,predictedlabel,pos_label=0,average='binary')\n",
    "    f1 = f1_score(truelabel,predictedlabel,pos_label=0, average='binary')\n",
    "    print(\"Precision for Benign apps \"+str(precise_score))\n",
    "    print(\"Recall for Benign apps \"+str(rec_score))\n",
    "    print(\"F1-score for Benign apps  \" + str(f1))\n",
    "    \n",
    "    \n",
    "    #plot the no-skill line too\n",
    "    positive_cases = sum(truelabel)/len(truelabel)\n",
    "    plt.plot([0, 1], [positive_cases, positive_cases], linestyle='--')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Precision-Recall-Curve-dynamic-'+str(counter)+'.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03333cf556291927c5f386415b154f59.csv 0ba28c10b8c36ab399990d7b54ffd8f6.csv\n"
     ]
    }
   ],
   "source": [
    "labels_skf = []\n",
    "files_ = os.listdir(input_path)\n",
    "files_.sort()\n",
    "for i,j in enumerate(files_):\n",
    "    if int(label_dict.get(j))==1:\n",
    "                labels_skf.append(1)\n",
    "    else:\n",
    "                labels_skf.append(0)\n",
    "len(labels_skf)\n",
    "print(files_[100],files_[332])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_actual_labels(file_names,validation_indices):\n",
    "    actual_labels=[]\n",
    "    for i,j in enumerate(validation_indices):\n",
    "        if int(label_dict.get(file_names[j]))==1:\n",
    "                    actual_labels.append(1)\n",
    "        else:\n",
    "                    actual_labels.append(0)\n",
    "    return actual_labels\n",
    "#print(actual_labels)\n",
    "#print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 12942 TEST: 2285\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4000, 100)         11100     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 128,606\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 11,100\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/7\n",
      "173/173 [==============================] - 836s 5s/step - loss: 0.4479 - acc: 0.7974\n",
      "Epoch 2/7\n",
      "173/173 [==============================] - 836s 5s/step - loss: 0.1414 - acc: 0.9348\n",
      "Epoch 3/7\n",
      "173/173 [==============================] - 832s 5s/step - loss: 0.0818 - acc: 0.9740\n",
      "Epoch 4/7\n",
      "173/173 [==============================] - 835s 5s/step - loss: 0.0499 - acc: 0.9875\n",
      "Epoch 5/7\n",
      "173/173 [==============================] - 834s 5s/step - loss: 0.0437 - acc: 0.9885\n",
      "Epoch 6/7\n",
      "173/173 [==============================] - 836s 5s/step - loss: 0.0394 - acc: 0.9902\n",
      "Epoch 7/7\n",
      "173/173 [==============================] - 832s 5s/step - loss: 0.0455 - acc: 0.9889\n",
      "TRAINING for FOLD  1  Finished\n",
      "Model_saved\n",
      "[1 0 0 ... 0 0 0]\n",
      "Confusion matrix, without normalization\n",
      "[[1589    3]\n",
      " [  50  643]]\n",
      "Area under graph 0.9831192153566777\n",
      "-------------------------------------------------\n",
      "Precision for Malicious apps 0.9953560371517027\n",
      "Recall for Malicious apps 0.9278499278499278\n",
      "F1-score for Malicious apps  0.9604182225541449\n",
      "-------------------------------------------------\n",
      "Precision for Benign apps 0.9694935936546675\n",
      "Recall for Benign apps 0.9981155778894473\n",
      "F1-score for Benign apps  0.9835964097802538\n",
      "ITERATION  1  FINISHED\n",
      "TRAIN: 12942 TEST: 2285\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4000, 100)         11100     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 128,606\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 11,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "173/173 [==============================] - 828s 5s/step - loss: 0.4891 - acc: 0.7743\n",
      "Epoch 2/7\n",
      "173/173 [==============================] - 823s 5s/step - loss: 0.4818 - acc: 0.7925\n",
      "Epoch 3/7\n",
      "173/173 [==============================] - 826s 5s/step - loss: 0.3555 - acc: 0.8579\n",
      "Epoch 4/7\n",
      "173/173 [==============================] - 821s 5s/step - loss: 0.4640 - acc: 0.7994\n",
      "Epoch 5/7\n",
      "173/173 [==============================] - 822s 5s/step - loss: 0.4403 - acc: 0.7857\n",
      "Epoch 6/7\n",
      "173/173 [==============================] - 822s 5s/step - loss: 0.5231 - acc: 0.7519\n",
      "Epoch 7/7\n",
      "173/173 [==============================] - 822s 5s/step - loss: 0.4997 - acc: 0.7684\n",
      "TRAINING for FOLD  2  Finished\n",
      "Model_saved\n",
      "[1 1 0 ... 0 0 1]\n",
      "Confusion matrix, without normalization\n",
      "[[1407  185]\n",
      " [ 336  357]]\n",
      "Area under graph 0.6339295269254763\n",
      "-------------------------------------------------\n",
      "Precision for Malicious apps 0.6586715867158671\n",
      "Recall for Malicious apps 0.5151515151515151\n",
      "F1-score for Malicious apps  0.5781376518218624\n",
      "-------------------------------------------------\n",
      "Precision for Benign apps 0.8072289156626506\n",
      "Recall for Benign apps 0.8837939698492462\n",
      "F1-score for Benign apps  0.8437781109445277\n",
      "ITERATION  2  FINISHED\n",
      "TRAIN: 12942 TEST: 2285\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 4000, 100)         11100     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 128,606\n",
      "Trainable params: 117,506\n",
      "Non-trainable params: 11,100\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "173/173 [==============================] - 825s 5s/step - loss: 0.5275 - acc: 0.7392\n",
      "Epoch 2/7\n",
      "173/173 [==============================] - 828s 5s/step - loss: 0.4059 - acc: 0.8235\n",
      "Epoch 3/7\n",
      "173/173 [==============================] - 824s 5s/step - loss: 0.4261 - acc: 0.7866\n",
      "Epoch 4/7\n",
      "173/173 [==============================] - 820s 5s/step - loss: 0.4304 - acc: 0.7872\n",
      "Epoch 5/7\n",
      "173/173 [==============================] - 828s 5s/step - loss: 0.3483 - acc: 0.8457\n",
      "Epoch 6/7\n",
      "173/173 [==============================] - 818s 5s/step - loss: 0.3497 - acc: 0.8429\n",
      "Epoch 7/7\n",
      "173/173 [==============================] - 821s 5s/step - loss: 0.3427 - acc: 0.8622\n",
      "TRAINING for FOLD  3  Finished\n",
      "Model_saved\n",
      "[0 1 0 ... 0 0 0]\n",
      "Confusion matrix, without normalization\n",
      "[[1491  101]\n",
      " [ 205  488]]\n",
      "Area under graph 0.8191113308382918\n",
      "-------------------------------------------------\n",
      "Precision for Malicious apps 0.8285229202037352\n",
      "Recall for Malicious apps 0.7041847041847041\n",
      "F1-score for Malicious apps  0.7613104524180967\n",
      "-------------------------------------------------\n",
      "Precision for Benign apps 0.879127358490566\n",
      "Recall for Benign apps 0.9365577889447236\n",
      "F1-score for Benign apps  0.9069343065693432\n",
      "ITERATION  3  FINISHED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skf = StratifiedShuffleSplit(n_splits=3, test_size=0.15, random_state=0)\n",
    "learning_rates= [0.01,0.001,0.0001]\n",
    "hidden_units = [128,128,128]\n",
    "counter = 0\n",
    "for train_indices, test_indices in skf.split(files_, labels_skf):\n",
    "    print(\"TRAIN:\", len(train_indices), \"TEST:\", len(test_indices))\n",
    "    train_data = get_content(files_,train_indices)\n",
    "    model = get_model(learning_rates[counter],hidden_units[counter])\n",
    "    steps_per_epoch = math.ceil(len(train_indices)/batch_size)\n",
    "    #steps_per_epoch =1\n",
    "    model.fit_generator(train_data,epochs=7,steps_per_epoch=steps_per_epoch)\n",
    "    print(\"TRAINING for FOLD \",counter+1,\" Finished\")\n",
    "    model.save('lstm_model_validation_'+str(counter)+\".h5\")\n",
    "    print('Model_saved')\n",
    "    validation_generator = get_validation_data(files_,test_indices)\n",
    "    prediction = model.predict_generator(validation_generator,math.ceil(len(test_indices)/validation_size))\n",
    "    prediction1 = np.argmax(prediction, axis=1) \n",
    "    print(prediction1)\n",
    "    actual_labels = create_actual_labels(files_,test_indices)\n",
    "    cm = confusion_matrix(actual_labels, prediction1)\n",
    "    plot_confusion_matrix(counter,cm,['benign','malicious'],title='CONFUSION MATRIX')\n",
    "    prediction_for_1=[]\n",
    "    for each_prediction in prediction:\n",
    "        prediction_for_1.append(each_prediction[1])\n",
    "    call_precision_recall_curve(counter,np.array(actual_labels),np.array(prediction1),np.array(prediction_for_1),1)\n",
    "    counter+=1\n",
    "    print('ITERATION ',counter,' FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
