{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Embedding,Dense,Flatten\n",
    "#from keras.models import load_model\n",
    "import numpy as np\n",
    "import os,sys\n",
    "import math\n",
    "import random\n",
    "seed_=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_static={}\n",
    "def create_static_label():\n",
    "    \n",
    "    with open(\"/home/ubuntu/composite.txt\") as file:\n",
    "        for _ in range(18227):\n",
    "            line  = file.readline().strip().split(\",\")\n",
    "            label_dict_static[line[0]] = line[1]\n",
    "create_static_label()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model_static = KeyedVectors.load(\"/home/ubuntu/first.model\", mmap='r')\n",
    "#word2vec_model_dynamic = KeyedVectors.load(\"/home/ubuntu/dynamic_word.model\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AACF906F90188F32700545FCA195CC045D3FF3A9257BB64CC841C7B8BB37B792.txt\n",
      "740585503e64a66f33e94f88b675b2f2.txt\n",
      "AD1939742F08BF7793842C629A1BE52884F242BB730C8A765A068A2B89CA12DA.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#nb_words=len(word2vec_model.wv.vocab), will be parameter for tokenizer\n",
    "tokenizer = Tokenizer(filters='#\\n')\n",
    "tokenizer.fit_on_texts(word2vec_model_static.wv.vocab.keys())\n",
    "word_index = tokenizer.word_index\n",
    "temp_array=[]\n",
    "validation_size = 50\n",
    "def get_validation_data(file_names):\n",
    "    c=0 \n",
    "    temp_array=[]\n",
    "    \n",
    "    while True:\n",
    "        if c>=len(file_names):\n",
    "            c=0\n",
    "        for i,j in enumerate(file_names[c:c+validation_size]):\n",
    "            l = open(\"/home/ubuntu/static_validation/\"+j,'r').read().strip()\n",
    "            padded_sequence =  sequence.pad_sequences(tokenizer.texts_to_sequences([l]),maxlen=4000,padding='post',truncating='post')\n",
    "            temp_array.append(padded_sequence[0])\n",
    "            \n",
    "\n",
    "        temp_array = np.array(temp_array)\n",
    "        #label_array = np.array(label_array)\n",
    "        #temp_array = temp_array[..., np.newaxis]\n",
    "        #print(\"\\nYIELDING FROM c = \",c,\" c+validation_size = \",c+validation_size,\" and length of temp_array = \",len(temp_array))\n",
    "        yield (temp_array)\n",
    "        temp_array=[]\n",
    "        c+=validation_size\n",
    "list_of_validation_files = os.listdir(\"/home/ubuntu/static_validation\")\n",
    "#print(list_of_validation_files)\n",
    "list_of_validation_files.sort()\n",
    "random.seed(a=seed_)\n",
    "random.shuffle(list_of_validation_files)\n",
    "print(list_of_validation_files[333])\n",
    "print(list_of_validation_files[2112])\n",
    "print(list_of_validation_files[454])\n",
    "validation_generator = get_validation_data(list_of_validation_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "static_model=load_model(\"/home/ubuntu/static-keras/lstm_model-final-mask_zero.h5\")\n",
    "#dynamic_model = load_model(\"/home/ubuntu/dynamic-keras/final-dynamic_lstm_model-maskZero.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_static_original = static_model.predict_generator(validation_generator,math.ceil(len(list_of_validation_files)/validation_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n",
      "2400\n",
      "400 2000\n"
     ]
    }
   ],
   "source": [
    "prediction_static=[]\n",
    "malicious_count=0\n",
    "desired_malicious_count=400\n",
    "index_array=[]\n",
    "mal_c=0\n",
    "ben_c=0\n",
    "for index, prediction in enumerate(prediction_static_original):\n",
    "    if int(label_dict_static.get(list_of_validation_files[index]))==1 and malicious_count<desired_malicious_count:\n",
    "        prediction_static.append(prediction)\n",
    "        index_array.append(index)\n",
    "        mal_c+=1\n",
    "        malicious_count+=1\n",
    "    if int(label_dict_static.get(list_of_validation_files[index]))==0:\n",
    "        prediction_static.append(prediction)\n",
    "        index_array.append(index)\n",
    "        ben_c+=1\n",
    "    \n",
    "print(len(prediction_static))\n",
    "print(len(index_array))\n",
    "print(mal_c,ben_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.clf()\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",verticalalignment=\"top\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Confusion-Matrix-Static-'+str(desired_malicious_count)+'png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.15612999, 0.84387   ], dtype=float32), array([0.9952608 , 0.00473927], dtype=float32), array([9.335336e-06, 9.999907e-01], dtype=float32)]\n",
      "[1 0 1 ... 0 0 0]\n",
      "Confusion matrix, without normalization\n",
      "[[1949   51]\n",
      " [  38  362]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "actual_labels=[]\n",
    "for i,j in enumerate(list_of_validation_files):\n",
    "    if i in index_array:\n",
    "        if int(label_dict_static.get(j))==1:\n",
    "                    actual_labels.append(1)\n",
    "        else:\n",
    "                    actual_labels.append(0)\n",
    "#print(actual_labels)\n",
    "#print(prediction)\n",
    "prediction=prediction_static\n",
    "#prediction = (np.asarray(prediction_dynamic)+np.asarray(prediction_static))/2\n",
    "print(prediction[:3])\n",
    "# print(prediction_dynamic[:3])\n",
    "# print(prediction_static[:3])\n",
    "prediction1 = np.argmax(prediction, axis=1) \n",
    "print(prediction1)\n",
    "\n",
    "cm = confusion_matrix(actual_labels, prediction1)\n",
    "plot_confusion_matrix(cm,['benign','malicious'],title='CONFUSION MATRIX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under graph 0.9511380003807398\n",
      "-------------------------------------------------\n",
      "Precision for Malicious apps 0.8765133171912833\n",
      "Recall for Malicious apps 0.905\n",
      "F1-score for Malicious apps  0.8905289052890528\n",
      "-------------------------------------------------\n",
      "Precision for Benign apps 0.980875691997987\n",
      "Recall for Benign apps 0.9745\n",
      "F1-score for Benign apps  0.9776774517180838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "from funcsigs import signature\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from inspect import signature\n",
    "\n",
    "def call_precision_recall_curve(truelabel,predictedlabel,actualprediction,label):\n",
    "    \n",
    "    #print(truelabel.shape)\n",
    "    #print(actualprediction.shape)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(truelabel,actualprediction)\n",
    "    au = auc(recall, precision)\n",
    "    print(\"Area under graph \"+str(au))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    rec_score = recall_score(truelabel,predictedlabel,pos_label=1,average='binary')\n",
    "    precise_score = precision_score(truelabel,predictedlabel,pos_label=1,average='binary')\n",
    "    f1 = f1_score(truelabel,predictedlabel,pos_label=1, average='binary')\n",
    "    print(\"Precision for Malicious apps \"+str(precise_score))\n",
    "    print(\"Recall for Malicious apps \"+str(rec_score))\n",
    "    print(\"F1-score for Malicious apps  \" + str(f1))\n",
    "    print(\"-------------------------------------------------\")\n",
    "    rec_score = recall_score(truelabel,predictedlabel,pos_label=0,average='binary')\n",
    "    precise_score = precision_score(truelabel,predictedlabel,pos_label=0,average='binary')\n",
    "    f1 = f1_score(truelabel,predictedlabel,pos_label=0, average='binary')\n",
    "    print(\"Precision for Benign apps \"+str(precise_score))\n",
    "    print(\"Recall for Benign apps \"+str(rec_score))\n",
    "    print(\"F1-score for Benign apps  \" + str(f1))\n",
    "    \n",
    "    \n",
    "    #plot the no-skill line too\n",
    "    positive_cases = sum(truelabel)/len(truelabel)\n",
    "    plt.plot([0, 1], [positive_cases, positive_cases], linestyle='--')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = ({'step': 'post'}\n",
    "                   if 'step' in signature(plt.fill_between).parameters\n",
    "                   else {})\n",
    "    plt.step(recall, precision, color='b', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Precision-Recall-Curve-Static'+str(desired_malicious_count)+'.png')\n",
    "    plt.clf()\n",
    "prediction_for_1=[]\n",
    "for each_prediction in prediction:\n",
    "    prediction_for_1.append(each_prediction[1])\n",
    "call_precision_recall_curve(np.array(actual_labels),np.array(prediction1),np.array(prediction_for_1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
